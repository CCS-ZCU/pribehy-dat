{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CCS-ZCU/pribehy-dat/blob/master/scripts/10_pdf.ipynb)\n",
    "\n",
    "# PDF: Extrakce textu\n",
    "\n",
    "**autor**: *Vojtěch Kaše* (kase@ff.zcu.cz)\n",
    "\n",
    "[![](https://ccs.zcu.cz/wp-content/uploads/2021/10/cropped-ccs-logo_black_space_240x240.png)](https://ccs.zcu.cz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9k-PhFNT-Eo"
   },
   "source": [
    "## Úvod a cíle kapitoly\n",
    "\n",
    "V této kapitole si ukážeme základní principy práce s PDF soubory. Formát PDF zavedl v roce 1993 John Warnock, spoluzakladatel společnosti Adobe. Cílem bylo nalézt způsob, jak by se dokumenty z jakékoli aplikace daly jednoduše ukládat, posílat v elektronickém formátu a prohlížet a tisknout na jakémkoli počítači, aniž by došlo ke změně jejich podoby. \n",
    "\n",
    "PDF formát je nyní standardem pro digitalizaci knih či archivních dokumentů. V tomto kontextu rozlišujeme zejména mezi PDF soubory s rozpoznanou textovou vrstvou a bez ní. Digitalizovaný dokument bez rozpoznané textové vrstvi je víceméně pouze seznam obrázků. PDF s rozpoznanou textovou vrstvou má kromě vrstvy obrázků ještě vrstvu textových prvků, tzv. textových bloků. Textový blok je entita, která sestává z dat ohledně svého geometrického postavení na stránce (typicky dva body vymezující rohy obdelníku) a sestavy znaků textu. Jedná-li se o PDF dokument, který vznikl např. převodem `.docx` souboru, lze očekávat, že textový obsah bude bezchybný. Jedná-li se však o soubor, který vznikl digitalizací analogového dokumentu, často zde narazíme na určité nedostatky spjaté s technologií OCR. Této technologii se budeme věnovat v samostatné kapitole. \n",
    "\n",
    "V následujícím cvičení budeme PDF soubory zpracovávat pomocí Python knihovny `PyMuPDF`, která se do Python prostředí importuje pod přezdívkou `fitz`. \n",
    "\n",
    "Toto cvičení je postaveno na textech zpřístupněných na stránkách [scriptum.cz](https://scriptum.cz). Tato webová platforma zpřístupňuje českou exilovou a samizdatovou literaturu z období komunismu. Jedná se o projekt Sdružení občanů Exodus v Plzni a Třemošné, který funguje od roku 2007. Digitalizace textů je náplní práce lidí se zdravotním handicapem v rámci chráněné dílny. Jedná se o unikátní kolekci několika set titulů a více než 11 tisíc souborů. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cvičení"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:16.680342Z",
     "start_time": "2024-01-24T14:44:16.648888Z"
    },
    "execution_time": 1
   },
   "outputs": [],
   "source": [
    "#!pip install PyMuPDF\n",
    "import fitz\n",
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Pro ukázku si nyní do našeho Python prostředí načteme jedno číslo exilového časopisu *Studie*, revue Křesťanské akademie.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:19.124804Z",
     "start_time": "2024-01-24T14:44:17.531946Z"
    },
    "execution_time": 2
   },
   "outputs": [],
   "source": [
    "url = \"https://scriptum.cz/soubory/scriptum/%5Bnode%5D/studie_1958_001_ocr_new.pdf\"\n",
    "pdf_object = io.BytesIO(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:19.126260Z",
     "start_time": "2024-01-24T14:44:19.047369Z"
    },
    "execution_time": 3
   },
   "outputs": [],
   "source": [
    "doc = fitz.open(\"pdf\", pdf_object.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:19.127660Z",
     "start_time": "2024-01-24T14:44:19.050134Z"
    },
    "execution_time": 4
   },
   "outputs": [],
   "source": [
    "doc.page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:19.211157Z",
     "start_time": "2024-01-24T14:44:19.174773Z"
    },
    "execution_time": 5
   },
   "outputs": [],
   "source": [
    "p = doc.load_page(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:19.930759Z",
     "start_time": "2024-01-24T14:44:19.890593Z"
    },
    "execution_time": 6
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:20.437879Z",
     "start_time": "2024-01-24T14:44:20.417889Z"
    },
    "execution_time": 7
   },
   "outputs": [],
   "source": [
    "pix = p.get_pixmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:21.299020Z",
     "start_time": "2024-01-24T14:44:21.254962Z"
    },
    "execution_time": 8
   },
   "outputs": [],
   "source": [
    "pix.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:22.012753Z",
     "start_time": "2024-01-24T14:44:21.972492Z"
    },
    "execution_time": 9
   },
   "outputs": [],
   "source": [
    "pix.height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Příslušný obrázek stránky má v tuto chvíli podobu matice či tabulky o 585 sloupcích a 769 řádcích. Co se však nachází v jednotlivých buňkách? Abychom to mohli blíže prozkoumat, data si ještě jednou prozatimně převedeme do standardního maticového objektu (tzv. `array`) knihovny `numpy`. A následně se podíváme na malý výřez dat pro několik pixelů:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:23.398583Z",
     "start_time": "2024-01-24T14:44:23.368224Z"
    },
    "execution_time": 10
   },
   "outputs": [],
   "source": [
    "np_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)\n",
    "np_array[400:404, 200:205]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Takto vypadají data pro celkově 20 pixelů. Jedná se o pixely z řádek 400 až 403 a sloupců 200 až 204 (pozor na indexování od nuly).\n",
    "\n",
    "Trojice číslic udává pro každý pixel jeho barvu ve standardu RGB, o kterým si můžeme přečíst více např. na wikipedii [zde](https://cs.wikipedia.org/wiki/RGB). Každé číslo může nabývat na hodnotě 0-255 a jednotlivé hodnoty odpovídají intenzitám červené (*R*), zelené (*G*) a modré (*B*). Černá barva je definována hodnotami (0, 0, 0), zatímco bílá hodnotami (255, 255, 255). V případě že se jedná o obrázek, který pochází z textového dokumentu na bílém pozadí, můžeme očekvávat, že velké množství pixelů bude nabývat hodnot (255,255,255). Tam, kde se naopak nacházejí nulové hodnoty, bude se jednat o černou. Tam, kde jsou hodnoty pro všechny tři barvy stejné, půjde o barvu na škále šedi, od úplné černé až po bílou. \n",
    "\n",
    "To jsou důležité vlastnosti, na kterých je postaveno velké množství algoritmů pro zpracování obrázků, které mají například za cíl zvýšit jejich kontrast apod. To je klíčové i pro potřeby rozpoznávání znaků (OCR), kterému se budeme věnovat níže."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nyní se však již podíváme na obrázek stránky jako takový. Můžeme jej vygenerovat přímo z maticových dat pomocí knihovny `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:25.776354Z",
     "start_time": "2024-01-24T14:44:25.635093Z"
    },
    "execution_time": 11
   },
   "outputs": [],
   "source": [
    "plt.imshow(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Snadno si zobrazíme např. pouze výřez této stránky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:29.016063Z",
     "start_time": "2024-01-24T14:44:28.918364Z"
    },
    "execution_time": 12
   },
   "outputs": [],
   "source": [
    "plt.imshow(np_array[410:730, 35:560])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nyní se vraťme k našemu objektu `p`, který reprezentuje veškerá data spojená s danou stránkou. Velice přímočarým způsobem získáme kompletní textový obsah:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:31.709807Z",
     "start_time": "2024-01-24T14:44:31.594383Z"
    },
    "execution_time": 13
   },
   "outputs": [],
   "source": [
    "print(p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pokročilejší způsob představuje vyjmout textová data ze stránky po jednotlivých textových blocích, které obsahují i informaci o svém geometrickém umístění uvnitř stránky:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:44:58.947208Z",
     "start_time": "2024-01-24T14:44:58.822728Z"
    },
    "execution_time": 14
   },
   "outputs": [],
   "source": [
    "p = doc.load_page(10)\n",
    "textblocks = p.get_text_blocks() # (\"blocks\")\n",
    "textblocks[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Geometrie zde vymezuje dva body ohraničující obdelník, v němž se text nachází:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:45:00.849423Z",
     "start_time": "2024-01-24T14:45:00.832280Z"
    },
    "execution_time": 15
   },
   "outputs": [],
   "source": [
    "rect = textblocks[2][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:45:01.546905Z",
     "start_time": "2024-01-24T14:45:01.380161Z"
    },
    "execution_time": 16
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np_array)\n",
    "patch = patches.Rectangle((rect[0], rect[1]),  # Bottom left corner\n",
    "                          rect[2] - rect[0],  # Width\n",
    "                          rect[3] - rect[1],  # Height\n",
    "                          linewidth=1, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Mít povědomí o těchto strukturálních vlastnostech PDF dokumentů je velice důležité, když z těchto dokumentů chceme získat strojově čitelný text pro další textové analýzy. PDF dokumenty jsou například často opatřeny záhlavím či zápatím, kde se objevuje třeba název periodika (jako je tomu zde), jméno autora, jméno příspěvku či číslo stránky. Vyextrahujeme-li ze všech stránek v daném dokumentu syrový text pomocí p.get_text(), budeme v našem textu mít i řetězce znaků z těchto textových bloků, což není žádoucí. Buď se tomu pokusíme předejít již při samotné extrakci, kdy můžeme využít geometrické polohy jednotlivých text bloků,  nebo se těchto dat pokusíme zbavit během následného čištění, např. pomocí *regulérních výrazů* (viz příslušná kapitola). Který případ je vhodnější závisí případ od případu a vyžaduje testování. Zde se budeme držet druhého případu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:45:23.370310Z",
     "start_time": "2024-01-24T14:45:23.028511Z"
    },
    "execution_time": 17
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for p in doc:\n",
    "    text += p.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:45.531081Z",
     "start_time": "2024-01-22T00:02:45.508409Z"
    },
    "execution_time": 18
   },
   "outputs": [],
   "source": [
    "text[5000:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Zkusme se zbavit všech záhlaví pomocí regexu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:46.286272Z",
     "start_time": "2024-01-22T00:02:46.267855Z"
    },
    "execution_time": 19
   },
   "outputs": [],
   "source": [
    "pattern = \"(_+)?(\\n?Studie[\\s_]+\\d{4}\\n?)(\\W*\\w{0,3}\\s*/\\s*\\d\\n?)?\"\n",
    "re.findall(pattern, text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:46.697789Z",
     "start_time": "2024-01-22T00:02:46.674976Z"
    },
    "execution_time": 20
   },
   "outputs": [],
   "source": [
    "text = re.sub(pattern, \"\\n\", text)\n",
    "text[3000:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Na text aplikujeme ještě několik čistících funkcí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:47.724829Z",
     "start_time": "2024-01-22T00:02:47.700315Z"
    },
    "execution_time": 21
   },
   "outputs": [],
   "source": [
    "text = re.sub(\"\\xad\\n\", \"\", text)\n",
    "text = text.replace(\"- \\n\", \"\")\n",
    "text = re.sub(\"\\s\\s+\", \" \", text.replace(\"\\n\", \" \"))\n",
    "text[10000:11000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Výsledek zdaleka není perfektní. V textu stále vidíme řadu problémů. Některé souvisejí s formátováním, jiné jsou dědictvím OCR analýzy. Pro naše aktuální potřeby však text v této podobě postačuje. \n",
    "\n",
    "Zkusme se však podívat na jiný text z téhož periodika:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:49.502774Z",
     "start_time": "2024-01-22T00:02:48.486339Z"
    },
    "execution_time": 22
   },
   "outputs": [],
   "source": [
    "url = \"https://scriptum.cz/soubory/scriptum/studie/studie_1978_055_ocr.pdf\"\n",
    "pdf_object = io.BytesIO(requests.get(url).content)\n",
    "doc = fitz.open(\"pdf\", pdf_object.read())\n",
    "p = doc.load_page(20) # vybereme stránku\n",
    "pix = p.get_pixmap()\n",
    "np_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)\n",
    "\n",
    "\n",
    "plt.imshow(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Vidíme, že text je nyní formátovaný odlišně. Například chybí původní záhlaví. Namísto toho vidíme v zápatí čísla stránek. V extrahovaném textu se pokusíme nyní zbavit i těch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:49.651707Z",
     "start_time": "2024-01-22T00:02:49.631601Z"
    },
    "execution_time": 23
   },
   "outputs": [],
   "source": [
    "p.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:50.158718Z",
     "start_time": "2024-01-22T00:02:50.151996Z"
    },
    "execution_time": 24
   },
   "outputs": [],
   "source": [
    "re.sub(\"\\n\\s?\\d{1,3}\\s?\\n\", \"\\n\", p.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:50.690582Z",
     "start_time": "2024-01-22T00:02:50.682398Z"
    },
    "execution_time": 25
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:51.396182Z",
     "start_time": "2024-01-22T00:02:51.212378Z"
    },
    "execution_time": 26
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for p in doc:\n",
    "    text += p.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:51.645062Z",
     "start_time": "2024-01-22T00:02:51.629853Z"
    },
    "execution_time": 27
   },
   "outputs": [],
   "source": [
    "pattern = \"(_+)?(\\n?Studie[\\s_]+\\d{4}\\n?)(\\W*\\w{0,3}\\s*/\\s*\\d\\n?)?\"\n",
    "text = re.sub(pattern, \"\\n\", text)\n",
    "text = re.sub(\"\\n\\s?\\d{1,3}\\s?\\n\", \"\\n\", text)\n",
    "text = re.sub(\"\\xad\\n\", \"\", text)\n",
    "text = text.replace(\"- \\n\", \"\")\n",
    "text = re.sub(\"\\s\\s+\", \" \", text.replace(\"\\n\", \" \"))\n",
    "text[10000:12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Výsledný text má daleko od dokonalosti. My se s ním však nyní spokojíme a postoupíme dále. Pomocí funkcí použitých výše vyexportujeme text ze všech čísel daného periodika. Použijeme k tomu webovou stránku rozcestníku, na kterém jsou všechny soubory dostupné v uspořádaném seznamu:\n",
    "\n",
    "https://scriptum.cz/soubory/scriptum/studie/\n",
    "\n",
    "Použijeme obdobný postup, jaký jsme použili v kapitole o webscrapingu. Nejprve si vyextrahujeme seznam jmen všech relevantních souborů (zajímají nás pouze souboru, které končí \"ocr.pdf\" - i zde se nám hodí regex).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:20:12.473707Z",
     "start_time": "2024-01-22T00:20:12.451794Z"
    },
    "execution_time": 28
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 29
   },
   "outputs": [],
   "source": [
    "chunks = re.findall(r'.{0,50000}\\.\\s', text, re.DOTALL)\n",
    "chunks_end = sum([len(chunk) for chunk in chunks])\n",
    "chunks.append(text[chunks_end:])\n",
    "docs = [nlp(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 30
   },
   "outputs": [],
   "source": [
    "[len(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 31
   },
   "outputs": [],
   "source": [
    "combined_text = \" \".join(doc.text for doc in docs)\n",
    "combined_doc = nlp(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 32
   },
   "outputs": [],
   "source": [
    "combined_doc = stanza.Document(sentences=[sentence for doc in docs for sentence in doc.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 33
   },
   "outputs": [],
   "source": [
    "combined_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:52.821459Z",
     "start_time": "2024-01-22T00:02:52.719242Z"
    },
    "execution_time": 34
   },
   "outputs": [],
   "source": [
    "resp = requests.get(\"https://scriptum.cz/soubory/scriptum/studie/\")\n",
    "soup = BeautifulSoup(resp.content)\n",
    "hrefs = [a.get('href') for a in soup.find_all(\"a\")]\n",
    "hrefs = [href for href in hrefs if re.search(\"ocr\\.pdf$\", href)]\n",
    "hrefs = [href for href in hrefs if not (\"rejstrik\" in href or \"obsah\" in href)]\n",
    "hrefs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:02:53.281394Z",
     "start_time": "2024-01-22T00:02:53.266547Z"
    },
    "execution_time": 35
   },
   "outputs": [],
   "source": [
    "len(hrefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nyní si vytvoříme funkci, do které vnoříme všechny extrakční, transformační a čistící procedury, které jsme prošli výše.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:05:35.396547Z",
     "start_time": "2024-01-22T00:05:35.328381Z"
    },
    "execution_time": 36
   },
   "outputs": [],
   "source": [
    "base_url = \"https://scriptum.cz/soubory/scriptum/studie/\"\n",
    "def get_cleaned_text(filename):\n",
    "    try:\n",
    "        url = base_url + filename\n",
    "        pdf_object = io.BytesIO(requests.get(url).content)\n",
    "        doc = fitz.open(\"pdf\", pdf_object.read())\n",
    "        text = \"\"\n",
    "        for p in doc:\n",
    "            text += p.get_text()\n",
    "        pattern = \"(_+)?(\\n?Studie[\\s_]+\\d{4}\\n?)(\\W*\\w{0,3}\\s*/\\s*\\d\\n?)?\"\n",
    "        text = re.sub(pattern, \"\\n\", text)\n",
    "        text = re.sub(\"\\n\\s?\\d{1,3}\\s?\\n\", \"\\n\", text)\n",
    "        text = re.sub(\"\\xad\\n\", \"\", text)\n",
    "        text = re.sub(\"\\s\\s+\", \" \", text.replace(\"\\n\", \" \"))\n",
    "        text = text.replace(\"- \\n\", \"\")\n",
    "        year = int(re.search(\"\\d{4}\", filename).group())\n",
    "        return filename, year, text\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A následně tuto funkci aplikujeme jeden po druhém na všechny dostupné soubory pomocí cyklu FOR. Máme před sebou více než 100 jmen souborů, tj. vzneseme více než 100 HTTP dotazů. Tudíž provedení kódu zabere nějaký čas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:07:04.117895Z",
     "start_time": "2024-01-22T00:05:37.978929Z"
    },
    "execution_time": 37
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scriptum_data = []\n",
    "for filename in hrefs:\n",
    "    filename, year, text = get_cleaned_text(filename)\n",
    "    scriptum_data.append({\"filename\" : filename, \"year\" : year, \"text\" : text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Vyextrahovaná data si převedeme do objektu typu `pandas.DataFrame` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:07:04.129398Z",
     "start_time": "2024-01-22T00:07:04.126418Z"
    },
    "execution_time": 38
   },
   "outputs": [],
   "source": [
    "scriptum_df = pd.DataFrame(scriptum_data)\n",
    "scriptum_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Spočteme počet znaků v každém z námi vyextrahovaných textů a vytvoříme nový sloupec \"n_chars\", kam tuto hodnotu uložíme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:07:04.167802Z",
     "start_time": "2024-01-22T00:07:04.133896Z"
    },
    "execution_time": 39
   },
   "outputs": [],
   "source": [
    "scriptum_df[\"n_chars\"] = scriptum_df[\"text\"].str.len()\n",
    "scriptum_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Díky tomu můžeme sečíst celkový počet znaků všech textů z daného periodika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:07:04.169806Z",
     "start_time": "2024-01-22T00:07:04.138756Z"
    },
    "execution_time": 40
   },
   "outputs": [],
   "source": [
    "scriptum_df[\"n_chars\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tímto se nám tedy dostal do rukou další nemalý dataset zajímavých kulturních dat. Pokud pracujeme s repozitoří \"pribehy-dat\" jako celkem, dataset si uložíme do podsložky data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T00:07:04.351197Z",
     "start_time": "2024-01-22T00:07:04.267601Z"
    },
    "execution_time": 41
   },
   "outputs": [],
   "source": [
    "scriptum_df.to_json(\"../data/scriptum_df.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Rozšiřující analýza: Zpracování textových dat\n",
    "\n",
    "Nyní trochu přeskočíme k tématu, kterým se budeme zabývat samostatně v jedné z následujících kapitol: kvantitativní textová analýza. Následující sérii kroků proto si proto v tuto chvíli nebudeme podrobně vysvětlovat, zaměříme se až na výsledná data.\n",
    "\n",
    "Budeme k nim potřebovat knihovnu stanza, kterou je třeba si doinstalovat pomocí příkazu `!pip install stanza` a model pro předzpracování textových dat v češtině."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:20:24.577847Z",
     "start_time": "2024-01-24T14:20:22.269403Z"
    },
    "execution_time": 42
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install stanza\n",
    "# !pip install wordcloud\n",
    "import stanza\n",
    "stanza.download(\"cs\")\n",
    "nlp = stanza.Pipeline(\"cs\")\n",
    "import pickle\n",
    "import nltk\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T13:55:49.111652Z",
     "start_time": "2024-01-24T13:55:48.884223Z"
    },
    "execution_time": 43
   },
   "outputs": [],
   "source": [
    "scriptum_df = pd.read_json(\"../data/scriptum_df.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pro testovací účely si vybereme jeden text z jednoho čísla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T13:55:59.441648Z",
     "start_time": "2024-01-24T13:55:59.235697Z"
    },
    "execution_time": 44
   },
   "outputs": [],
   "source": [
    "text = scriptum_df[\"text\"].tolist()[0]\n",
    "text[2000:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Na celý dokument aplikujeme jazykový model pro předzpracování, který text automaticky:\n",
    "* rozdělí do vět\n",
    "* věty do slov\n",
    "* jednotlivým slovům přiřadí *lemmata*, tj. převede je do tvarů, jak je najdeme ve slovníku (např. \"je\" -> \"být\").\n",
    "* přiřadí jim \"part-of-speech\" (POS) tagy (např. \"NOUN\", \"VERB\" apod. \"PUNCT\" apod.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T11:56:40.383351Z",
     "start_time": "2024-01-22T11:56:40.348666Z"
    },
    "execution_time": 45
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T11:32:48.730213Z",
     "start_time": "2024-01-22T11:32:09.291129Z"
    },
    "execution_time": 46
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Vytvořili jsme nový `stanza` objekt `doc`, který obsahuje podrobně jazykově anotovanou reprezentaci celého textu. Z této reprezentace si nyní vyjmeme pouze lemmata vybraných slovních druhů: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T11:32:48.742146Z",
     "start_time": "2024-01-22T11:32:48.737553Z"
    },
    "execution_time": 47
   },
   "outputs": [],
   "source": [
    "lemmatized_sents = []\n",
    "for sent in doc.sentences:\n",
    "    lemmatized_sents.append([t.lemma for t in sent.words if t.upos in [\"PROPN\", \"NOUN\", \"VERB\", \"ADJ\"]])\n",
    "print(lemmatized_sents[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ani v tomto případě nejsou výsledky ani zdaleka perfektní. Vše se odvíjí zejména z kvality vstupních dat. Vidíme např., že model si nedokáže poradit se slovy, které jsou ve stupních datech zachycena v rozdělené podobě apod. I přesto nyní postoupíme dále a aplikujeme danou proceduru na texty všech čísel. \n",
    "\n",
    "Opět si pro tento účel nadefinujeme speciální funkci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T12:11:23.536552Z",
     "start_time": "2024-01-22T12:11:23.244823Z"
    },
    "execution_time": 48
   },
   "outputs": [],
   "source": [
    "processed = []\n",
    "!mkdir ../data/large_files\n",
    "!mkdir ../data/large_files/lemsents\n",
    "def get_lemmatized_sentences(filename, text, n_chars):\n",
    "    if filename not in [\"studie_1990_132_ocr.pdf\"]:\n",
    "        if filename + \".pickle\" not in os.listdir(\"../data/large_files/lemsents/\"):\n",
    "            chunks = re.findall(r'.{0,50000}\\.\\s', text, re.DOTALL)\n",
    "            chunks_end = sum([len(chunk) for chunk in chunks])\n",
    "            chunks.append(text[chunks_end:])\n",
    "            docs = [nlp(chunk) for chunk in chunks]\n",
    "            lemmatized_sents = []\n",
    "            for doc in docs:\n",
    "                for sent in doc.sentences:\n",
    "                    lemmatized_sents.append([t.lemma for t in sent.words if t.upos in [\"PROPN\", \"NOUN\", \"VERB\", \"ADJ\"]])\n",
    "            processed.append(filename)\n",
    "            pathfn = \"../data/large_files/lemsents/\" + filename + \".pickle\"\n",
    "            with open(pathfn, 'wb') as f:\n",
    "                pickle.dump(lemmatized_sents, f)\n",
    "            print(filename, n_chars, len(processed))\n",
    "        else:\n",
    "            pathfn = \"../data/large_files/lemsents/\" + filename + \".pickle\"\n",
    "            with open(pathfn, 'rb') as f:\n",
    "                lemmatized_sents = pickle.load(f)\n",
    "    else:\n",
    "        lemmatized_sents = None\n",
    "    return lemmatized_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Aplikace této funkce na všechny dokumenty v seznamu však zabere značný čas. Abychom se vyhnuli čekání, načteme si proto data, v kterých jsem již tuto proceduru aplikoval dříve (jak jsem to provedl je vidět v zakomentovaných příkazech v buňce níže."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T13:57:30.640169Z",
     "start_time": "2024-01-24T13:57:29.250726Z"
    },
    "execution_time": 49
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#scriptum_df[\"lemmatized_sents\"] = scriptum_df.apply(lambda row: get_lemmatized_sentences(row[\"filename\"], row[\"text\"], row[\"n_chars\"]), axis=1)\n",
    "# scriptum_df.to_json(\"../data/large_files/scriptum_df_lemmata.json\")\n",
    "scriptum_df = pd.read_json(\"../data/large_files/scriptum_df_lemmata.json\")\n",
    "scriptum_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Nejčastější slova po obdobích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T16:13:11.015719Z",
     "start_time": "2024-01-24T16:13:09.610844Z"
    },
    "execution_time": 50
   },
   "outputs": [],
   "source": [
    "periods_freqs = {}\n",
    "periods = [(1958,1968), (1969,1976), (1977,1990)]\n",
    "periods_labels = [\"Studie {0}-{1}\".format(str(period[0]), str(period[1])) for period in periods]\n",
    "for period, period_label in zip(periods, periods_labels):\n",
    "    subset_df = scriptum_df[scriptum_df[\"year\"].between(period[0], period[1])]\n",
    "    lemmatized_sents = [sentences for sentences in subset_df[\"lemmatized_sents\"] if sentences != None]\n",
    "    sentences_flat = [sent for file_sents in lemmatized_sents for sent in file_sents]\n",
    "    lemmata_list = [lemma for sent in sentences_flat for lemma in sent]\n",
    "    lemmata_list = [lemma for lemma in lemmata_list if len(lemma) > 1]\n",
    "    lemmata_freqs = nltk.FreqDist(lemmata_list).most_common()\n",
    "    periods_freqs[period_label] = lemmata_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Podívejme se namátkou na 50 nejčastějších slov z nejranějšího období."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:21:21.206127Z",
     "start_time": "2024-01-24T14:21:21.201794Z"
    },
    "execution_time": 51
   },
   "outputs": [],
   "source": [
    "periods_freqs[periods_labels[0]][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T16:13:40.405675Z",
     "start_time": "2024-01-24T16:13:40.164125Z"
    },
    "execution_time": 52
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=400).generate_from_frequencies(dict(periods_freqs[periods_labels[0]][:50]))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:29:55.225269Z",
     "start_time": "2024-01-24T14:29:53.582834Z"
    },
    "execution_time": 53
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "fig, axs = plt.subplots(2,2, figsize=(4.5, 3) , dpi=300, tight_layout=True)\n",
    "for item, ax in zip(periods_freqs.items(), axs.ravel()):    \n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(dict(item[1][:n]))\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(item[0])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lematizované věty s vybranými slovy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nyní si nadefinujeme funkci, pomocí které budeme moci vyextrahovat veškeré věty obsahující konkrétní slova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:30:20.049265Z",
     "start_time": "2024-01-24T14:30:20.014782Z"
    },
    "execution_time": 54
   },
   "outputs": [],
   "source": [
    "def extract_target_sents(lemmatized_sents, targets):\n",
    "    try:\n",
    "        return [sent for sent in lemmatized_sents if any(target in sent for target in targets)]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:30:20.734264Z",
     "start_time": "2024-01-24T14:30:20.574433Z"
    },
    "execution_time": 55
   },
   "outputs": [],
   "source": [
    "my_targets = [\"kultura\"]\n",
    "scriptum_df[\"target_sents\"] = scriptum_df[\"lemmatized_sents\"].apply(extract_target_sents, args=(my_targets,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:30:21.264661Z",
     "start_time": "2024-01-24T14:30:21.217543Z"
    },
    "execution_time": 56
   },
   "outputs": [],
   "source": [
    "scriptum_df[\"target_sents\"].apply(lambda x: len(x) if x != None else 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:30:21.796977Z",
     "start_time": "2024-01-24T14:30:21.767062Z"
    },
    "execution_time": 57
   },
   "outputs": [],
   "source": [
    "scriptum_df[\"target_sents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Jaká slova se v těchto větách objevují nejčastěji?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 58
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:31:56.126268Z",
     "start_time": "2024-01-24T14:31:56.011120Z"
    },
    "execution_time": 59
   },
   "outputs": [],
   "source": [
    "column_sents = scriptum_df[\"target_sents\"].tolist()\n",
    "target_sents_counts = nltk.FreqDist([t for sent in [t for sent in column_sents for t in sent] for t in sent]) #\n",
    "target_sents_counts = target_sents_counts.most_common()\n",
    "target_sents_counts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-24T14:32:27.222021Z",
     "start_time": "2024-01-24T14:32:26.986998Z"
    },
    "execution_time": 60
   },
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(dict(target_sents_counts[:50]))\n",
    "plt.imshow(wc) # , interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 61
   },
   "outputs": [],
   "source": [
    "column_sents = scriptum_df[\"target_sents\"].tolist()\n",
    "target_sents_counts = nltk.FreqDist([t for sent in [t for sent in column_sents for t in sent] for t in sent]) #\n",
    "target_sents_counts = target_sents_counts.most_common()\n",
    "target_sents_counts[:20]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pribehy_kernel",
   "language": "python",
   "name": "pribehy_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
