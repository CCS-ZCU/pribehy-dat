{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayU0ucI1UYjq"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CCS-ZCU/pribehy-dat/blob/master/scripts/nlp.ipynb)\n",
    "\n",
    "# NLP: Zpracování přirozeného jazyka\n",
    "\n",
    "**autor**: *Vojtěch Kaše* (kase@ff.zcu.cz)\n",
    "\n",
    "[![](https://ccs.zcu.cz/wp-content/uploads/2021/10/cropped-ccs-logo_black_space_240x240.png)](https://ccs.zcu.cz)\n",
    "\n",
    "## Úvod a cíle kapitoly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qReidPvUbhS"
   },
   "source": [
    "Cílem tohoto cvičení je provést základní kvantitativní textovou analýzu některého digitalizovaného dokumentu z **Archivu Jana Patočky** ([AJP](https://archiv.janpatocka.cz/items/browse?tags=fulltext)). Omezíme se však pouze na dokumenty, u kterých je dostupný digitální přepis (tzv. fulltext). Tj. URL adresa, kterou hledáme, je adresa jakéhokoliv námi vybraného dokumentu z daného archivu pro který je dostupný přepis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cvičení"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T11:52:25.891723Z",
     "start_time": "2024-04-20T11:52:15.157350Z"
    },
    "execution_time": 1
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install stanza\n",
    "!pip install gensim\n",
    "import stanza\n",
    "stanza.download(\"cs\")\n",
    "czech_pipeline = stanza.Pipeline(\"cs\")\n",
    "import requests\n",
    "from urllib.request import urlopen # pro práci w webovými adresami\n",
    "from bs4 import BeautifulSoup # pro práci s webovými stránkami ve formátu html\n",
    "import pandas as pd # pro práci s tabulkami ve formátu dataframe\n",
    "import nltk # modul pro práci s textovými daty\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('punkt')\n",
    "import matplotlib.pyplot as plt # modul pro vytváření grafů\n",
    "import numpy as np # modul pro pokročilejší matematické operace\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQMW7GIgiLep"
   },
   "source": [
    "Váš hlavní úkol je spojen s buňkou níže. V ní je potřeba nahradit obsah proměnné \"url\", tj. **vyměnit webovou adresu jednoho dokumentu z AJP za adresu jiného dokumentu z téhož archivu**.  Pozor, že webová adresa musí být uvnitř uvozovek. Aby se změna projevila, je třeba buňku nakonec spustit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:04.284164Z",
     "start_time": "2024-03-13T14:50:04.020620Z"
    },
    "execution_time": 2,
    "id": "_wNGwo_kI8Ih"
   },
   "outputs": [],
   "source": [
    "url = \"https://archiv.janpatocka.cz/items/show/308\"\n",
    "web_text = urlopen(url).read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(web_text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:06.664194Z",
     "start_time": "2024-03-13T14:50:04.851453Z"
    },
    "execution_time": 3
   },
   "outputs": [],
   "source": [
    "# Tato buňka slouží ke kontrole průchodu tímto cvičením. \n",
    "# Pokud toto cvičení plníte v rámci svých studijních povinností na ZČU, buňku spusťte a držte se instrukcí.\n",
    "exec(requests.get(\"https://sciencedata.dk/shared/856b0a7402aa7c7258186a8bdb329bd3?download\").text)\n",
    "kontrola_pruchodu(ntb=\"nlp\", arg1=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:07.892174Z",
     "start_time": "2024-03-13T14:50:07.887635Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution_time": 4,
    "id": "7G4gz4Ats6IM",
    "outputId": "b12035e8-c294-46ee-a4c1-cfeaeeefd2c1"
   },
   "outputs": [],
   "source": [
    "text_title = soup.find(\"div\", id=\"item_title\").get_text()\n",
    "text_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:08.366412Z",
     "start_time": "2024-03-13T14:50:08.362038Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution_time": 5,
    "id": "9C9_3lxwtRN8",
    "outputId": "4a58139a-7df1-4d25-8361-cb5633102125"
   },
   "outputs": [],
   "source": [
    "[div for div in soup.find_all(\"div\", class_=\"col span_7_of_9\")][3].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:08.781379Z",
     "start_time": "2024-03-13T14:50:08.777876Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution_time": 6,
    "id": "mkA4UEOhKG3e",
    "outputId": "86862efb-5167-4b43-db88-c6a5b3ad942d"
   },
   "outputs": [],
   "source": [
    "text_dokumentu = soup.find(\"div\", id=\"trans_full\").get_text()\n",
    "text_dokumentu = \" \".join(text_dokumentu.split())\n",
    "print(text_dokumentu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:09.305709Z",
     "start_time": "2024-03-13T14:50:09.301718Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution_time": 7,
    "id": "5Buh6M5GOIS2",
    "outputId": "f562fdb6-d745-446d-8bb3-79f58edefd93"
   },
   "outputs": [],
   "source": [
    "# dokument jako list slov získáme pomocí funkce \"split()\"\n",
    "# uložíme si ho takto do nové proměnné \"string_list\"\n",
    "string_list = text_dokumentu.split()\n",
    "# prvních 20 prvků tohoto listu si nyní vypíšeme:\n",
    "string_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:09.922847Z",
     "start_time": "2024-03-13T14:50:09.918365Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution_time": 8,
    "id": "f_8KiHMZ3cl0",
    "outputId": "6421edae-0c72-4f8c-cb62-e2f00ac34647"
   },
   "outputs": [],
   "source": [
    "### pomocí funkce \"len()\" spočítáme délku tohoto listu slov:\n",
    "len(string_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgIbCh5v4FR2"
   },
   "source": [
    "### Lematizace a postagging\n",
    "\n",
    "S textem článku, tak jak se nyní nachází v proměnné \"text_clanku\", bychom se ale při kvantitativní textové analýze stále příliš daleko nedostali. Čeština je totiž morfologicky velice bohatý jazyk. Chceme-li např. spočítat kolikrát se v textu objevuje sloveso \"mít\", s textem v aktuální podobě se příliš daleko nedstaneme. Zde potřebujeme na naše textová data aplikovat dvě další procedury:\n",
    "\n",
    "\n",
    "1.   lemmatizace, tj. převedení slov z textu do jejich základních tvarů (slovesa do infinitivu, podstatná jména do 1.pádu singuláru apod.)\n",
    "2.   POS-tagging (\"part-of-speech tags\"),  tj. určení slovních druhů a mluvnických kategorií\n",
    "\n",
    "Aplikace těchto procedur nám umožní získat data z hlediska kvantitativní textové analýzy výrazně zajímavější.\n",
    "\n",
    "V případě češtiny se můžeme v tomto případě opřít o model pro jazykový model pro zpracování češtiny vyvinutý pro knihovnu [stanza](https://stanfordnlp.github.io/stanza/), konkrétně [stanza-cs](https://huggingface.co/stanfordnlp/stanza-cs). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T11:54:41.892651Z",
     "start_time": "2024-04-20T11:54:41.837957Z"
    },
    "execution_time": 9
   },
   "outputs": [],
   "source": [
    "sent = \"Česká republika je velmi sebevědomý stát, který dokáže stát na vlastních nohách.\"\n",
    "doc = czech_pipeline(sent)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:12.705034Z",
     "start_time": "2024-03-13T14:50:11.230119Z"
    },
    "execution_time": 10
   },
   "outputs": [],
   "source": [
    "# vytvoříme morfoligicky zpracovanou verzi našeho textu a konvenčně si ji uložíme do proměnně `doc`.  \n",
    "doc = czech_pipeline(text_dokumentu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tímto jsme vytvořili morfoligicky zpracovanou verzi našeho textu a konvenčně si ji uložili do proměnně `doc`. Tento objekt nyní neobsahuje pouze syrový text, ale také text rozdělený do vět, každou větu na jednotlivá slova a každému slovo je automaticky přiřazeno jeho *lemma*, morfologické určení a některé další atributy. Na tuto datovou strukturou se můžeme nyní pracovat např. následujícím způsobem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:12.715959Z",
     "start_time": "2024-03-13T14:50:12.706003Z"
    },
    "execution_time": 11
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "  for token in sentence.words:\n",
    "    data.append({\n",
    "      'sent_n' : i,\n",
    "      'text': token.text,\n",
    "      'lemma': token.lemma,\n",
    "      'upos': token.upos,\n",
    "      'xpos': token.xpos\n",
    "    })\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df[30:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obdobně můžeme vybrat z vět pouze lemmata slov vybraných slovních druhů:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:13.722498Z",
     "start_time": "2024-03-13T14:50:13.719348Z"
    },
    "execution_time": 12
   },
   "outputs": [],
   "source": [
    "lemmatized_sentences = []\n",
    "for sent in doc.sentences:\n",
    "  sentence_lemmata = []\n",
    "  for token in sent.words:\n",
    "    if token.upos in [\"PROPN\", \"NOUN\", \"VERB\", \"ADJ\"]:\n",
    "      sentence_lemmata.append(token.lemma)\n",
    "  lemmatized_sentences.append(sentence_lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:14.259285Z",
     "start_time": "2024-03-13T14:50:14.254697Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution_time": 13,
    "id": "TFA6qawWPTeF",
    "outputId": "bc7c059c-243c-4ff2-96a7-2c84e18cbe7d"
   },
   "outputs": [],
   "source": [
    "lemmatized_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZELHnjgDCobV"
   },
   "source": [
    "V této podobě může být již vcelku zajímavé podívat se na frekvence výskytů slov, resp. jejich lematizovaných tvarů. K tomu použijeme funkce z modulu \"nltk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:15.248174Z",
     "start_time": "2024-03-13T14:50:15.245463Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution_time": 14,
    "id": "X4U0o6kLCn1b",
    "outputId": "e4cc6856-4c1b-4419-9f9e-3c169870344f"
   },
   "outputs": [],
   "source": [
    "# nejprve si z naší filtrované tabulky vyxtrahujeme lemmata samotná\n",
    "lemmata = [l for s in lemmatized_sentences for l in s]\n",
    "# pro každý jednotlivý výraz necháme spočítat jeho počet výskytů\n",
    "lemmata_freq = nltk.FreqDist(lemmata)\n",
    "# vybereme např. 10 nejfrekventovanějších slov (rozumějme lemmatizovaných substantiv, adjektiv a sloves)\n",
    "lemmata_most_freq = lemmata_freq.most_common(10)\n",
    "print(lemmata_most_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní si tato data vizualizujeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:17.586086Z",
     "start_time": "2024-03-13T14:50:17.426403Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "execution_time": 15,
    "id": "DWv__cRRGjR6",
    "outputId": "8413317b-6492-46d3-9cdb-eccdd3084e44"
   },
   "outputs": [],
   "source": [
    "# kvůli horizontálnímu zobrazení prohodíme pořadí na našem listu\n",
    "lemmata_mostfreq = lemmata_most_freq\n",
    "lemmata_mostfreq.reverse()\n",
    "\n",
    "# pro potřeby grafu přiřadíme hodnoty jednotlivým osám\n",
    "height = [tup[1] for tup in lemmata_mostfreq]\n",
    "bars = [tup[0] for tup in lemmata_mostfreq]\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "plt.barh(y_pos, height)\n",
    "# graf si pojmenujeme a osu také\n",
    "plt.yticks(y_pos, bars)\n",
    "plt.xlabel('Frekvence výskytů')\n",
    "plt.title('Frekvence výskytů nejčastějších slov')\n",
    "# graf si zobrazíme\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přehled nejfrekventovanějších slov jistě představuje cennou informaci o obsahu zkoumaného textu či souboru textů. Neříká však nic o tom, jakým způsobem jsou zde příslušná slova použita. K tomu potřebujeme zaprvé a především informaci o tom, která slova se spolu a jak často spoluvyskytují. Tuto informaci získáme tak, že zkonstruujeme tzv. **kookurenční matici**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:20.540744Z",
     "start_time": "2024-03-13T14:50:20.531152Z"
    },
    "execution_time": 16
   },
   "outputs": [],
   "source": [
    "vocabulary = [tup[0] for tup in lemmata_freq.most_common(100)]\n",
    "vect = CountVectorizer(vocabulary=vocabulary, lowercase=False)\n",
    "X = vect.fit_transform([\" \".join(sent) for sent in lemmatized_sentences])\n",
    "Xc = (X.T * X)  # This is the matrix manipulation step\n",
    "cooccurrence_matrix_df = pd.DataFrame(Xc.todense(), index=vocabulary, columns=vocabulary)\n",
    "cooccurrence_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:25.781867Z",
     "start_time": "2024-03-13T14:50:21.347147Z"
    },
    "execution_time": 17
   },
   "outputs": [],
   "source": [
    "kontrola_pruchodu(ntb=\"nlp\", arg1=vocabulary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud jsme se dostali až jsem, hlavní část našeho úkolu byla splněna. Máme-li však čas a chuť, můžeme pokračovat dále."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Zde končí povinná část cvičení.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozšiřující analýza: Analýza všech děl z Archivu Jana Patočky "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:28.307403Z",
     "start_time": "2024-03-13T14:50:28.304195Z"
    },
    "execution_time": 18
   },
   "outputs": [],
   "source": [
    "def get_patocka(url):\n",
    "  textdata = {}\n",
    "  try:\n",
    "    web_text = urlopen(url).read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(web_text, \"html.parser\")\n",
    "    text_title = soup.find(\"div\", id=\"item_title\").get_text()\n",
    "    date = [div for div in soup.find_all(\"div\", class_=\"col span_7_of_9\")][3].get_text()\n",
    "    text_dokumentu = soup.find(\"div\", id=\"trans_full\").get_text()\n",
    "    text_dokumentu = \" \".join(text_dokumentu.split())\n",
    "    #print(id)\n",
    "    textdata[\"url\"] = url\n",
    "    textdata[\"title\"] = text_title\n",
    "    textdata[\"date\"] = date\n",
    "    textdata[\"rawtext\"] = text_dokumentu\n",
    "  except:\n",
    "    pass\n",
    "  return textdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Získejme zdrojový kód jedné stránky soupisu přepsaných textů a získejme URL odkazy všech dokumentů, na které odkazuje: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:30.028888Z",
     "start_time": "2024-03-13T14:50:29.467706Z"
    },
    "execution_time": 19
   },
   "outputs": [],
   "source": [
    "base_url = \"https://archiv.janpatocka.cz/items/browse?tags=fulltext&page=\"\n",
    "page_url = base_url + str(2)\n",
    "resp_text = requests.get(page_url).text\n",
    "[\"https://archiv.janpatocka.cz\" + href for href in re.findall(\"/items/show/\\d+\", resp_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuto proceduru nyní zopakujeme pro všechny zbývající stránky:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T11:15:36.948157Z",
     "start_time": "2024-03-13T11:15:22.228066Z"
    },
    "execution_time": 20
   },
   "outputs": [],
   "source": [
    "docs_urls = []\n",
    "base_url = \"https://archiv.janpatocka.cz/items/browse?tags=fulltext&page=\"\n",
    "for page_n in range(1,34):\n",
    "  page_url = base_url + str(page_n)\n",
    "  resp_text = requests.get(page_url).text\n",
    "  page_urls = [\"https://archiv.janpatocka.cz\" + href for href in re.findall(\"/items/show/\\d+\", resp_text)]\n",
    "  docs_urls.extend(page_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kolik odkazů jsme získali?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T13:15:29.761102Z",
     "start_time": "2024-03-13T13:15:29.756993Z"
    },
    "execution_time": 21
   },
   "outputs": [],
   "source": [
    "len(docs_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T13:19:41.147759Z",
     "start_time": "2024-03-13T13:19:41.143644Z"
    },
    "execution_time": 22
   },
   "outputs": [],
   "source": [
    "docs_urls = list(set(docs_urls))\n",
    "len(docs_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T13:19:54.549311Z",
     "start_time": "2024-03-13T13:19:54.546985Z"
    },
    "execution_time": 23
   },
   "outputs": [],
   "source": [
    "docs_urls[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní můžeme získat přepisy textů ze všech těchto stránek. Tato procedura však zabere značný čas. V rámci výuky ji tudíž můžeme přeskočit a načteme místo toho již předpřipravená data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T13:57:11.832025Z",
     "start_time": "2024-03-13T13:57:11.765631Z"
    },
    "execution_time": 24
   },
   "outputs": [],
   "source": [
    "#data = []\n",
    "#for url in docs_urls:\n",
    "#  textdata = get_patocka(url)\n",
    "#  data.append(textdata)\n",
    "#patocka_all_df = pd.DataFrame(data)\n",
    "#patocka_all_df = patocka_all_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textová data ve sloupci `rawtext` si nyní můžeme lemmatizovat pomocí funkce níže. I tato procedura je však v případě celého korpusu relativně výpočetně náročná, a proto ji nyní také přeskočíme a načteme již lemmatizovaná data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:00:11.210790Z",
     "start_time": "2024-03-13T14:00:11.207369Z"
    },
    "execution_time": 25
   },
   "outputs": [],
   "source": [
    "def get_lemmatized_sents(rawtext):\n",
    "  try:\n",
    "    doc = czech_pipeline(rawtext)\n",
    "    lemmatized_sentences = []\n",
    "    for sent in doc.sentences:\n",
    "        sentence_lemmata = []\n",
    "        for token in sent.words:\n",
    "            if token.upos in [\"PROPN\", \"NOUN\", \"VERB\", \"ADJ\"]:\n",
    "                sentence_lemmata.append(token.lemma)\n",
    "        lemmatized_sentences.append(sentence_lemmata)\n",
    "    return lemmatized_sentences\n",
    "  except:\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:28:35.068094Z",
     "start_time": "2024-03-13T14:28:35.007663Z"
    },
    "execution_time": 26
   },
   "outputs": [],
   "source": [
    "#patocka_all_df[\"lemmatized_sents\"] = patocka_all_df[\"rawtext\"].apply(get_lemmatized_sents)\n",
    "# patocka_all_df.to_json(\"../data/patocka_all_df.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 27
   },
   "outputs": [],
   "source": [
    "patocka_all_df = pd.read_json(\"https://raw.githubusercontent.com/CCS-ZCU/pribehy-dat/master/data/patocka_all_df.json\")\n",
    "patocka_all_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:50:58.133008Z",
     "start_time": "2024-03-13T14:50:58.127916Z"
    },
    "execution_time": 28
   },
   "outputs": [],
   "source": [
    "patocka_all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:51:01.267154Z",
     "start_time": "2024-03-13T14:51:01.143622Z"
    },
    "execution_time": 29
   },
   "outputs": [],
   "source": [
    "all_sents = [sent for doc in patocka_all_df[\"lemmatized_sents\"] for sent in doc]\n",
    "docs = [\" \".join(sent) for sent in all_sents]\n",
    "lemmata_flat = [t for s in all_sents for t in s]\n",
    "lemmata_freqs = nltk.FreqDist(lemmata_flat).most_common()\n",
    "vocabulary = [tup[0] for tup in lemmata_freqs][:500]\n",
    "lemmata_freqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:51:02.480198Z",
     "start_time": "2024-03-13T14:51:02.312118Z"
    },
    "execution_time": 30
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(vocabulary=vocabulary, lowercase=False)\n",
    "X = vect.fit_transform(docs)\n",
    "Xc = (X.T * X)  # This is the matrix manipulation step\n",
    "cooccurrence_matrix_df = pd.DataFrame(Xc.todense(), index=vocabulary, columns=vocabulary)\n",
    "cooccurrence_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:51:05.729402Z",
     "start_time": "2024-03-13T14:51:05.379293Z"
    },
    "execution_time": 31
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100)  # choose the number of components\n",
    "X_reduced = svd.fit_transform(Xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:51:06.896118Z",
     "start_time": "2024-03-13T14:51:06.883553Z"
    },
    "execution_time": 32
   },
   "outputs": [],
   "source": [
    "kv = KeyedVectors(vector_size=X_reduced.shape[1])\n",
    "for word, vector in zip(vocabulary, X_reduced):\n",
    "  kv.add_vector(word, vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T14:51:54.481413Z",
     "start_time": "2024-03-13T14:51:54.408377Z"
    },
    "execution_time": 33
   },
   "outputs": [],
   "source": [
    "kv.most_similar(\"svět\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": 34
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pribehy_kernel",
   "language": "python",
   "name": "pribehy_kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
